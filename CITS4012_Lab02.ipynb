{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LDY681/CITS4401/blob/master/CITS4012_Lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHPkKbuhPF6"
      },
      "source": [
        "# Lab 02\n",
        "\n",
        "Today we will investigate some word representation models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ0HLTPGemjV"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNyhgK5QTOuD",
        "outputId": "5952f42b-db06-473e-f3a4-21f337dff3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree\n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmae7urS8RHD"
      },
      "source": [
        "### Download data from Google Drive\n",
        "For today's lab we will download and use the TED script data from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7vMHSahdnf"
      },
      "source": [
        "#### Google Drive Access Setup\n",
        "Running the following code will generate a link and a field for entering a verification code.\n",
        "\n",
        "Click the link, which will direct to the Google Sign In page. Sign in with your own Google account by following the instructions on the page.\n",
        "\n",
        "Then copy the generated verification code from the page into the verification code field and press Enter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTSQtnPkfyzj"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAbjQzThnT5"
      },
      "source": [
        "#### Downloading TED Scripts from Google Drive\n",
        "Click on left side \"Files\" tab and see the file is downloaded successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVk7tjwvhl-6"
      },
      "outputs": [],
      "source": [
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('ted_en-20160408.xml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIPpEvI4kqMV"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYmEQgB7XoDE",
        "outputId": "34fdcb8c-1044-43f3-e66f-35d5c526ffc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ]
        }
      ],
      "source": [
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=[]\n",
        "for parse_text in target_text.xpath('//content/text()'):\n",
        "  # Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "  content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "  sent_text.extend(sent_tokenize(content_text))\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CojV1MbhkQxK"
      },
      "source": [
        "### Word2Vec - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLq1VIZ7TDog"
      },
      "source": [
        "For more details about gensim.models.word2vec you can refer to [API for Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW1iEee3lZC9"
      },
      "outputs": [],
      "source": [
        "# Initialize and train a word2vec model with the following parameters:\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "# vector_size: dimensionality of the word vectors\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "# workers: Use specified number of worker threads to train the model (=faster training with multicore machines)\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=2, sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FKp3X7pkRm6",
        "outputId": "87c75d8e-203a-4878-ccf6-94a4cfb2abef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8587272763252258),\n",
            " ('guy', 0.8291348814964294),\n",
            " ('boy', 0.7908284664154053),\n",
            " ('lady', 0.7581247687339783),\n",
            " ('girl', 0.7467600107192993),\n",
            " ('gentleman', 0.7287656664848328),\n",
            " ('poet', 0.6999704241752625),\n",
            " ('kid', 0.6936692595481873),\n",
            " ('rabbi', 0.6747317910194397),\n",
            " ('soldier', 0.6702558398246765)]\n"
          ]
        }
      ],
      "source": [
        "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
        "# Get the top 10 similar words to 'man' by calling most_similar()\n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model\n",
        "\n",
        "similar_words= wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
        "pprint.pprint(similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9201pniCRJ5",
        "outputId": "bf317d9d-47f0-45bf-8592-36be07b3b43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.60685945, -1.4720893 , -0.5555122 ,  0.41853294,  3.0359192 ,\n",
              "        0.66410536, -0.19398196, -0.25500894, -0.21476606, -0.9031569 ,\n",
              "       -1.271061  , -1.3551382 , -0.69903153,  0.76567477, -0.22641864,\n",
              "       -0.23364457,  0.08249884,  0.34372026,  1.0979712 , -0.47126678,\n",
              "        0.14631775,  0.76350653, -0.36029148, -0.6420899 ,  1.105928  ,\n",
              "       -0.5739546 , -1.4254571 , -0.23970161, -0.11716913, -1.4127717 ,\n",
              "       -0.6327663 , -0.19428213,  1.120359  ,  0.29552048, -0.4403368 ,\n",
              "       -0.7781586 , -1.7749039 , -0.95066226, -1.7194873 ,  0.58914924,\n",
              "        1.24877   , -2.237428  , -0.9308997 ,  1.1748303 ,  0.3051784 ,\n",
              "       -0.57309157, -0.5592564 , -1.5808377 , -0.48706755, -0.63506067,\n",
              "       -0.35635212, -1.8064479 , -0.01820517,  0.42595696, -0.15961033,\n",
              "       -0.31712747, -0.0802803 , -0.41574925, -1.3154297 , -0.3400672 ,\n",
              "       -0.91761494, -0.8314774 ,  0.8798532 ,  1.1056008 , -1.8123772 ,\n",
              "        0.50630265, -1.0136482 , -0.6793732 , -0.54858965,  1.877079  ,\n",
              "        0.49439323,  0.02380179,  1.6188636 , -0.9726749 , -0.5164705 ,\n",
              "       -2.0440705 ,  0.40803537,  0.37214044, -0.5128553 ,  2.05579   ,\n",
              "        0.1758397 , -0.74288595, -0.5717327 , -0.3159922 , -0.7018558 ,\n",
              "        0.15582328,  0.02159307,  0.5210172 , -1.0617176 , -0.36981863,\n",
              "       -0.06658887, -0.05992882,  0.39960808, -0.1440584 ,  2.7819605 ,\n",
              "        0.01726903,  1.1438719 ,  0.7378964 , -3.438702  ,  2.809026  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Get the actual values of the word embedding\n",
        "print(wv_cbow_model.wv['man'].shape)\n",
        "wv_cbow_model.wv['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsFHg0znlPSf"
      },
      "source": [
        "### Word2Vec - Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k16AowhCWUXu"
      },
      "outputs": [],
      "source": [
        "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8UiVfr2cBtA",
        "outputId": "69905057-d3dc-41fb-a3d6-3702a302d819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('guy', 0.7486716508865356),\n",
            " ('woman', 0.7414814829826355),\n",
            " ('boy', 0.6883013248443604),\n",
            " ('rabbi', 0.6865651607513428),\n",
            " ('soldier', 0.6806363463401794),\n",
            " ('imam', 0.6686133742332458),\n",
            " ('girl', 0.6666482090950012),\n",
            " ('waitress', 0.6642106771469116),\n",
            " ('gentleman', 0.6600041389465332),\n",
            " ('son', 0.6584417223930359)]\n"
          ]
        }
      ],
      "source": [
        "similar_words = wv_sg_model.wv.most_similar(\"man\")\n",
        "pprint.pprint(similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7hpsEniCWNP",
        "outputId": "e3026de1-e979-451d-fcb4-034ad6bad4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.01929773, -0.11699665,  0.02510257, -0.3900924 ,  0.05746577,\n",
              "       -0.32545704,  0.22499293,  0.01624372, -0.37282705, -0.11967955,\n",
              "        0.06645289, -0.31855267,  0.11150549,  0.12174855,  0.13609643,\n",
              "       -0.01315264, -0.09146716, -0.27764595,  0.09273223, -0.6474919 ,\n",
              "        0.36845762,  0.05285766,  0.3048615 , -0.02727229,  0.07570012,\n",
              "       -0.32143363, -0.1943806 , -0.5393496 , -0.35529333, -0.23425893,\n",
              "        0.02169297,  0.10467848,  0.3058368 , -0.23348494,  0.11132445,\n",
              "        0.00895701, -0.46932402, -0.15702084, -0.19303252, -0.03460472,\n",
              "        0.24047117, -0.5997824 , -0.20956261, -0.06851273,  0.05618289,\n",
              "        0.0591498 , -0.28936312, -0.1588873 ,  0.11550762,  0.4702715 ,\n",
              "        0.06012606, -0.5756171 ,  0.0591196 ,  0.29488853, -0.4819099 ,\n",
              "       -0.46851996, -0.3065339 , -0.233695  , -0.77354515, -0.30411208,\n",
              "        0.08958799,  0.07706858, -0.00522187,  0.04294226, -0.2981597 ,\n",
              "        0.20716266, -0.20438708, -0.032197  , -0.29116872, -0.14087288,\n",
              "        0.06387384,  0.0408737 ,  0.27453035,  0.02839157,  0.2043202 ,\n",
              "       -0.06851164, -0.30178756,  0.09808215, -0.3154724 ,  0.44368115,\n",
              "       -0.06596053, -0.09453094, -0.36281115,  0.2800718 , -0.659383  ,\n",
              "       -0.12694207,  0.1437788 ,  0.26413888, -0.04200149, -0.2148897 ,\n",
              "        0.17464797,  0.00908581,  0.3501424 ,  0.3804176 ,  0.53562343,\n",
              "        0.17158371,  0.22589765,  0.15184979, -0.6788883 ,  0.11359326],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Get the actual values of the word embedding\n",
        "print(wv_sg_model.wv['man'].shape)\n",
        "wv_sg_model.wv['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfF7YqvpppbG"
      },
      "source": [
        "## Word2Vec vs FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8IV7D6VAEcr"
      },
      "source": [
        "Word2Vec - Skip Gram cannot find similar words to \"electrofishing\" as \"electrofishing\" is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "oS9c2uWWquWG",
        "outputId": "a83a74c0-4a97-4916-c561-8cfa0af99991"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a2f7ec57b31e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv_sg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
          ]
        }
      ],
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TpkScI8sA9G"
      },
      "source": [
        "### FastText - Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAqOR1Vqps6M"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqkvyiUw_DRh"
      },
      "outputs": [],
      "source": [
        "# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n",
        "ft_sg_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv26QObJriB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af734f92-7f92-4f1a-ccde-fee1316c43fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('electrolux', 0.8434932231903076),\n",
            " ('electrolyte', 0.8434127569198608),\n",
            " ('electroshock', 0.8383511304855347),\n",
            " ('electro', 0.8373796939849854),\n",
            " ('electroencephalogram', 0.8315842747688293),\n",
            " ('electrochemical', 0.8223231434822083),\n",
            " ('airbag', 0.8195294141769409),\n",
            " ('electrogram', 0.8184046745300293),\n",
            " ('electric', 0.8128581047058105),\n",
            " ('electromagnet', 0.8105154633522034)]\n"
          ]
        }
      ],
      "source": [
        "# As we can see, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Qjz_MYCjfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6616722a-b47d-497a-e70b-a5916e622096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.50047988e-01, -6.89602867e-02,  3.89394671e-01, -2.76936352e-01,\n",
              "        1.41633540e-01,  1.94506139e-01,  4.92269248e-01,  1.09998822e-01,\n",
              "        5.78278974e-02,  2.52662748e-01, -6.29469275e-01, -3.53289656e-02,\n",
              "       -3.07683527e-01,  7.87398756e-01,  7.92795643e-02, -2.35458776e-01,\n",
              "        2.41862983e-01,  3.04099023e-01,  1.55633479e-01, -3.55820388e-01,\n",
              "       -4.97786731e-01, -6.50473952e-01, -2.35346720e-01,  3.07153434e-01,\n",
              "       -6.15150221e-02,  1.22725822e-01, -4.85381395e-01, -4.19013090e-02,\n",
              "        2.19789639e-01, -7.03777745e-02, -3.39940041e-01,  2.47025982e-01,\n",
              "       -1.49779603e-01,  1.57967713e-02,  2.55649269e-01,  1.57415852e-01,\n",
              "       -4.74274158e-04, -4.44329351e-01, -2.56455123e-01, -1.17389075e-01,\n",
              "        3.32507670e-01, -4.05092984e-01, -1.35794401e-01, -1.08681582e-01,\n",
              "        1.38201877e-01,  2.28947133e-01, -1.22739911e-01, -5.97184300e-01,\n",
              "       -2.39729024e-02, -1.55293480e-01, -1.55925348e-01, -5.38983583e-01,\n",
              "        2.76638065e-02, -6.58500195e-02, -2.32562855e-01, -8.00388381e-02,\n",
              "        2.66597539e-01, -6.07917309e-01, -1.81457430e-01, -9.34524462e-02,\n",
              "       -2.49008626e-01, -2.84318179e-01,  1.53849334e-01, -2.27001920e-01,\n",
              "       -3.00224245e-01,  6.10461049e-02,  2.54677206e-01, -1.70022652e-01,\n",
              "       -1.83284074e-01,  9.31032076e-02, -7.53011480e-02,  6.61722600e-01,\n",
              "       -2.83318698e-01,  2.07802996e-01,  7.91053772e-02, -1.74406424e-01,\n",
              "        2.77610093e-01,  4.70817089e-02, -4.51073110e-01,  4.89614218e-01,\n",
              "       -3.00520658e-02, -1.16219215e-01,  1.05000906e-01, -2.31239628e-02,\n",
              "       -1.15952738e-01,  3.45835127e-02,  5.35369217e-01, -5.70317805e-01,\n",
              "       -1.61141410e-01,  1.99795172e-01, -1.45579979e-01, -3.05495501e-01,\n",
              "       -1.86007902e-01,  4.01452750e-01,  3.01213682e-01,  1.14565551e-01,\n",
              "       -3.68355244e-01, -6.18351065e-02, -3.59128892e-01,  1.91677772e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Get the actual values of the word embedding\n",
        "print(ft_sg_model.wv['man'].shape)\n",
        "ft_sg_model.wv['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x2aQpfsFSx"
      },
      "source": [
        "### FastText - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUBqvqpc2sbL"
      },
      "outputs": [],
      "source": [
        "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
        "ft_cbow_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=2, sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUj1RUzM2nLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19a9f92-ebc7-41c4-86ce-58da08ab4c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('licensing', 0.9190741777420044),\n",
            " ('fishing', 0.9163623452186584),\n",
            " ('recycling', 0.9082537293434143),\n",
            " ('transplanting', 0.9065425395965576),\n",
            " ('planting', 0.9061746001243591),\n",
            " ('flourishing', 0.9019662141799927),\n",
            " ('operating', 0.9014682173728943),\n",
            " ('skateboarding', 0.9014284610748291),\n",
            " ('filtering', 0.8999918699264526),\n",
            " ('refreshing', 0.8990527391433716)]\n"
          ]
        }
      ],
      "source": [
        "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmrYQASiCqR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08791fc6-8a98-4a9c-e4fc-b33d9ad6907c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5425415 , -1.1754454 ,  1.6317027 , -1.8820904 ,  2.6224658 ,\n",
              "       -3.3683383 ,  3.1499622 ,  0.68869096,  0.5023509 ,  1.0023242 ,\n",
              "        0.27640718, -0.219519  , -1.0553201 ,  1.7997074 , -0.31267235,\n",
              "        1.11762   ,  2.951331  ,  1.2837464 ,  1.6842574 , -1.6547748 ,\n",
              "        0.47951406, -3.3914952 ,  0.29087505, -2.577944  , -1.6098778 ,\n",
              "        0.75627357, -0.5521012 ,  0.7146278 , -0.9032083 , -0.33916712,\n",
              "        0.17855282, -1.6272317 ,  3.1951585 ,  2.4263668 ,  1.2513207 ,\n",
              "       -1.3610299 ,  1.9772518 , -3.597067  ,  0.45941162,  2.9057949 ,\n",
              "       -2.4691849 , -3.427454  , -2.444344  ,  0.8545365 ,  0.76271373,\n",
              "       -1.9802312 , -0.3197431 , -1.7366194 , -2.6800025 ,  0.81547636,\n",
              "        0.3761957 , -5.175985  , -1.3547659 ,  1.5462291 ,  1.2253296 ,\n",
              "        1.4600092 , -0.13631634, -0.29946953, -0.55912817,  0.93046254,\n",
              "        1.3275    , -1.955591  ,  2.9287431 ,  2.2828186 ,  1.3954879 ,\n",
              "        0.3383766 , -0.01674332, -3.0262206 ,  0.47083744,  2.0959518 ,\n",
              "       -2.606406  ,  3.226274  , -1.7213266 , -1.6439272 , -0.19943525,\n",
              "        0.13516752,  2.6024518 ,  2.3986871 , -0.5960228 ,  4.3682256 ,\n",
              "       -1.1123896 ,  0.08125006, -0.31802005,  0.75149304, -1.9475305 ,\n",
              "        0.32534334, -0.16629383, -2.991035  , -1.4492786 ,  3.7783387 ,\n",
              "       -1.398836  ,  1.3800848 ,  3.5186756 ,  0.26144722, -0.64082915,\n",
              "        0.21824597,  1.413179  , -0.2907315 , -1.3125124 ,  0.15341373],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Get the actual values of the word embedding\n",
        "print(ft_cbow_model.wv['man'].shape)\n",
        "ft_cbow_model.wv['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hjmOhmRi7Ov"
      },
      "source": [
        "## King - Man + Woman = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw7b9OSwjGm0"
      },
      "source": [
        "Try both CBOW and Skip Gram model to calculate \"King - Man + Woman = ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovTXjSdgrw36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49471352-b773-49ae-b69e-d2a9ebe10d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('president', 0.7947894334793091)]\n"
          ]
        }
      ],
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters to create a word expression\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUtbE2jwq1to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df54847c-a62a-4570-a759-00e1db15147a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6636009812355042)]\n"
          ]
        }
      ],
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PWf2I4_WZpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf338660-5e9d-41b1-f588-aa9e92595060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('kidding', 0.8935132026672363)]\n"
          ]
        }
      ],
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9x51rRhWZrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d9ceee-5788-4314-95a4-13860a31cdac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pauling', 0.7013661861419678)]\n"
          ]
        }
      ],
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpAd8t-wjTMA"
      },
      "source": [
        "This is not what we expected...Probably not enough data to answer as \"queen\"\n",
        "\n",
        "Let's try with a larger sized training data (Google has already trained Word2Vec with Google News data) in the following section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMY5w8F7rElp"
      },
      "source": [
        "## Using Pretrained word embeddings with Gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bxDq30F3lj7q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keivkY13L4Nz"
      },
      "source": [
        "### 1.Download and load from Google pretrained Word2Vec binary file\n",
        "[Link to Project](https://code.google.com/archive/p/word2vec/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teQvZDSirVVC"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained vectors trained on part of Google News dataset (about 100 billion words)\n",
        "# Beware, this file is big (3.39GB) - might be long waiting!\n",
        "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':id2})\n",
        "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTrXl4FRMitm"
      },
      "outputs": [],
      "source": [
        "# Uncompress the downloaded file\n",
        "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64e_sRJ1rhUa"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Load the pretrained vectors with KeyedVectors instance\n",
        "# Note that we set the limit=100000 here, which means we set a maximum number of word-vectors to read from the file, to avoid out of memory issue and load vectors faster.\n",
        "\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "gn_wv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvMQp2-Tr3zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c8a666-2a1c-4cce-f345-30a3b36338b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519)]\n"
          ]
        }
      ],
      "source": [
        "# Now we can try to calculate \"King - Man + Woman = ?\" again\n",
        "result = gn_wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9N64tY4RNlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f7ca68-0677-48a7-fe2a-daba30b2a65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n",
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5332662]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Let's also try to extract the word embeddings and check their shape\n",
        "wv_banana = gn_wv_model[\"banana\"]\n",
        "wv_avocado = gn_wv_model[\"avocado\"]\n",
        "print(wv_banana.shape)\n",
        "print(wv_avocado.shape)\n",
        "\n",
        "# We can also calculate the cosine similarity ourselves with the extracted words\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity([wv_banana],[wv_avocado])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOgbNjggC7_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272c3a08-0257-4107-9839-10064f5ce177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
              "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
              "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
              "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
              "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
              "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
              "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
              "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
              "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
              "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
              "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
              "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
              "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
              "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
              "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
              "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
              "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
              "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
              "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
              "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
              "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
              "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
              "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
              "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
              "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
              "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
              "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
              "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
              "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
              "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
              "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
              "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
              "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
              "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
              "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
              "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
              "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
              "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
              "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
              "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
              "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
              "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
              "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
              "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
              "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
              "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
              "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
              "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
              "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
              "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
              "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
              "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
              "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
              "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
              "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
              "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
              "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
              "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
              "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
              "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Get the actual value of the word embedding\n",
        "print(gn_wv_model['man'].shape)\n",
        "gn_wv_model['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ws7QvPMq9s"
      },
      "source": [
        "### 2.Load a pretrained word embedding model using API\n",
        "The following code illustrates another way of loading pretrained word embeddings with Gensim. Here we try with GloVe embedding trained on Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvAP4nyYM_qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657ae403-e7bc-4e4c-f2a0-82d289c55172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "0.95908207\n",
            "0.040917932987213135\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# download the model and return as object ready for use\n",
        "model = api.load(\"glove-twitter-25\")\n",
        "# The similarity() function can calculate the cosine similarity between two given words\n",
        "print(model.similarity(\"cat\",\"dog\"))\n",
        "# The distance() function is another way of calculating the similarity between two given words, which returns (1 - cosine similarity) instead\n",
        "print(model.distance(\"cat\",\"dog\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn0_uRZfDLcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76092ac-1bf7-48ed-d3ee-01d9242e4f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.37013 , -0.39648 , -0.021712, -0.6301  , -0.3189  ,  0.34329 ,\n",
              "        0.10968 ,  0.4879  , -0.48663 ,  0.36837 , -0.39179 ,  0.25414 ,\n",
              "       -4.9282  ,  0.067597,  0.37147 ,  0.36817 ,  1.1655  ,  0.092116,\n",
              "       -0.87735 , -0.74562 ,  0.40903 ,  1.5672  , -0.23879 ,  0.24755 ,\n",
              "        0.76386 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Get the actual value of the word embedding\n",
        "print(model['man'].shape)\n",
        "model['man']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLruu6247Ze"
      },
      "source": [
        "# [Tips] Play with Colab Form Fields\n",
        "**The Form** supports multiple types of fields, including **input fields**, **dropdown menus**.\n",
        "\n",
        "In Lab1 E1, we already used the input fields. Let's try more now. You can edit this section by double-clicking it.\n",
        "\n",
        "Let's get familiar by changing the value in each input field (on the right) and checking the changes in the code (on the left) - and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBNvQmee5QIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f417085-d00c-48eb-ffed-dc2bf8c69638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string is examples\n",
            "slider_value 164\n"
          ]
        }
      ],
      "source": [
        "#@title Example form fields\n",
        "#@markdown please put description\n",
        "\n",
        "string = 'examples'  #@param {type: \"string\"}\n",
        "slider_value = 164  #@param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  #@param {type: \"number\"}\n",
        "date = '2023-02-17'  #@param {type: \"date\"}\n",
        "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#print the output\n",
        "print(\"string is\",string)\n",
        "print('slider_value',slider_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfESzUvhCZOt"
      },
      "source": [
        "# Lab 2 Exercise\n",
        "Please complete the following **question E1** for Lab 2 and check with your Lab Facilitator in the lab.\n",
        "\n",
        "**Submission Due**: Your Lab 3 (in Week 4)\n",
        "\n",
        "**Submission Method**: Show your final code to your Lab Facilitator during the Lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DupJY3rOcozM"
      },
      "source": [
        "## E1. Let's find synonyms\n",
        "Let's assume the cosine similarity, or distance, between two word embedding vectors can indicate if the words are semantically similar to each other. In this exercise, you will implement a function called find_synonym(), in which:\n",
        "\n",
        "1. A list of 6 words are given\n",
        "2. You need to implement your own algorithm to find the **synonym for each of the words (i.e. words with the highest cosine similarity or smallest distance)** in the list **from the rest of 5 words** based on the cosine similarity calculated. (Using the .similarity() or distance() function from *Load pretrained word embedding model using API* section above may help)\n",
        "3. Print out the synonyms found\n",
        "\n",
        "Please use the pretrained 50-dimensional GloVe word embedding trained on wikipedia and gigaword corpus. (You can use the gensim.downloader to load by passing 'glove-wiki-gigaword-50' to the .load() function, refer to the *Load pretrained word embedding model using API* section above)\n",
        "\n",
        "Before the function, you may need to import any required libraries.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCd23jclUGYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3e8dc4-deae-4bcf-fe99-613168358cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dog', 'cat'), ('cat', 'dog'), ('person', 'people'), ('computer', 'laptop'), ('people', 'person'), ('laptop', 'computer')]\n"
          ]
        }
      ],
      "source": [
        "# E1. Let's find synonyms\n",
        "# Let's assume the cosine similarity, or distance, between two word embedding vectors can indicate if the words are semantically similar to each other. In this exercise, you will implement a function called find_synonym(), in which:\n",
        "\n",
        "# A list of 6 words are given\n",
        "# You need to implement your own algorithm to find the synonym for each of the words (i.e. words with the highest cosine similarity or smallest distance) in the list from the rest of 5 words based on the cosine similarity calculated. (Using the .similarity() or distance() function from Load pretrained word embedding model using API section above may help)\n",
        "# Print out the synonyms found\n",
        "# Please use the pretrained 50-dimensional GloVe word embedding trained on wikipedia and gigaword corpus. (You can use the gensim.downloader to load by passing 'glove-wiki-gigaword-50' to the .load() function, refer to the Load pretrained word embedding model using API section above)\n",
        "\n",
        "# Before the function, you may need to import any required libraries.\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Complete the following function based on the requirements above\n",
        "\n",
        "# The list of words to find synonyms\n",
        "words = [\"dog\", \"cat\", \"person\", \"computer\", \"people\", \"laptop\"]\n",
        "\n",
        "# The dictionary that stores the highest similarity value to each keywords\n",
        "# @ param key: word\n",
        "# @ param value: [synonym1, cosine similarity]\n",
        "dict = {\n",
        "  \"dog\": [],\n",
        "  \"cat\": [],\n",
        "  \"person\": [],\n",
        "  \"computer\": [],\n",
        "  \"people\": [],\n",
        "  \"laptop\": []\n",
        "}\n",
        "\n",
        "# Load GloVe\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "def find_synonyms():\n",
        "  for word in words:\n",
        "    find_synonym(word)\n",
        "\n",
        "def find_synonym(word):\n",
        "  for candidate in [x for x in words if x != word]:\n",
        "    similarity = model.similarity(word, candidate)\n",
        "    # if the dict entry is empty, push\n",
        "    if dict[word] == []:\n",
        "      dict[word] = [candidate, similarity]\n",
        "    # elif compare the entry with current similarity, and update\n",
        "    else:\n",
        "      if similarity > dict[word][1]:\n",
        "        dict[word] = [candidate, similarity]\n",
        "\n",
        "def print_synonyms():\n",
        "  synonyms = [(dictKey, dict[dictKey][0]) for dictKey in dict]\n",
        "  print(synonyms)\n",
        "\n",
        "# Call the function to get the synonyms and print out the synonyms for each word\n",
        "find_synonyms()\n",
        "print_synonyms()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E1 Sample Output**\n",
        "The following is the sample output for E1 - Please just only check the output format, i.e. your output DO NOT need to be exactly the same, as long as you fullfill all the E1 requirements given above."
      ],
      "metadata": {
        "id": "xs10-S7N5W7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sample_output.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jPOnoUCh5eEY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrjbuZYrXD88"
      },
      "source": [
        "# Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWjBxLdTcEi"
      },
      "source": [
        "## Word Embedding Visual Inspector (WEVI)\n",
        "If you would like to visualise how Word2Vec is learning, the following link is useful https://ronxin.github.io/wevi/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}